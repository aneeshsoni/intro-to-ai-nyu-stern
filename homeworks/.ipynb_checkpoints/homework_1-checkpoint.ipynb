{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: (0, 0, 1)\n",
      "Output: X4 = 1, X5 = 3, X6 = 2, X7 = 1.000\n",
      "\n",
      "Input: (0, 1, 0)\n",
      "Output: X4 = 0, X5 = 3, X6 = 1, X7 = 1.000\n",
      "\n",
      "Input: (1, 0, 0)\n",
      "Output: X4 = 3, X5 = 0, X6 = 1, X7 = 0.047\n",
      "\n",
      "Input: (1, 1, 1)\n",
      "Output: X4 = 4, X5 = 4, X6 = 4, X7 = 1.000\n",
      "\n",
      "Input: (1, -1, 1)\n",
      "Output: X4 = 4, X5 = 0, X6 = 2, X7 = 0.119\n",
      "\n",
      "Input: (1, 1, -1)\n",
      "Output: X4 = 2, X5 = 0, X6 = 0, X7 = 0.018\n",
      "\n",
      "Input: (-1, -1, 1)\n",
      "Output: X4 = 0, X5 = 2, X6 = 0, X7 = 0.982\n",
      "\n",
      "Input: (-1, -1, -1)\n",
      "Output: X4 = 0, X5 = 0, X6 = 0, X7 = 0.500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_network(x1, x2, x3):\n",
    "    # ReLU activation function\n",
    "    def relu(x):\n",
    "        return max(0, x)\n",
    "    \n",
    "    # Sigmoid activation function\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    # Calculate X4 (ReLU)\n",
    "    x4 = relu(3 * x1 + 0 * x2 + 1 * x3)\n",
    "    \n",
    "    # Calculate X5 (ReLU)\n",
    "    x5 = relu(-2 * x1 + 3 * x2 + 3 * x3)\n",
    "    \n",
    "    # Calculate X6 (ReLU)\n",
    "    x6 = relu(1 * x1 + 1 * x2 + 2 * x3)\n",
    "    \n",
    "    # Calculate X7 (Sigmoid)\n",
    "    x7 = sigmoid(-2 * x4 + 2 * x5 + 3 * x6)\n",
    "    \n",
    "    return x4, x5, x6, x7\n",
    "\n",
    "input_values = [\n",
    "    (0, 0, 1),\n",
    "    (0, 1, 0),\n",
    "    (1, 0, 0),\n",
    "    (1, 1, 1),\n",
    "    (1, -1, 1),\n",
    "    (1, 1, -1),\n",
    "    (-1, -1, 1),\n",
    "    (-1, -1, -1)\n",
    "]\n",
    "\n",
    "for inputs in input_values:\n",
    "    x4, x5, x6, x7 = calculate_network(*inputs)\n",
    "    print(f\"Input: {inputs}\")\n",
    "    print(f\"Output: X4 = {x4}, X5 = {x5}, X6 = {x6}, X7 = {x7:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under supervised learning, the algorithm is given a set of input-output pairs and learns to map inputs to outputs by adjusting its parameters. In the case of a neural network, the algorithm adjusts the weights and biases of the network to minimize the error between the predicted output and the true output. One way this happens is in the training process is by \"randomly\" initializing the weights and biases, and then using an optimization algorithm to adjust the weights and biases so that the error is minimized. Another way this happens is by using gradient descent to update the weights and biases in the direction of the negative gradient of the loss function with respect to the weights and biases. This can also happen in forward or backward propagation ....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part b)\n",
    "Let's call the top hidden layer H1, the bottom hidden layer H2 and the calculated output layer O1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.75\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_network(x1, x2, expected_output, w1, w2, w3, w4, w5, w6):\n",
    "    # ReLU activation function\n",
    "    def relu(x):\n",
    "        return max(0, x)\n",
    "    \n",
    "    H1 = relu(w1 * x1 + w3 * x2)\n",
    "    H2 = relu(w4 * x1 + w2 * x2)\n",
    "\n",
    "    O1 = w5 * H1 + w6 * H2\n",
    "    squared_error = (O1 - expected_output)**2\n",
    "    \n",
    "    return(squared_error)\n",
    "\n",
    "input_values = [\n",
    "    (0, 0, 0),\n",
    "    (0, 1, 1),\n",
    "    (1, 0, 1),\n",
    "    (1, 1, 0)\n",
    "]\n",
    "\n",
    "total_squared_errors = 0\n",
    "\n",
    "for inputs in input_values:\n",
    "    total_squared_errors += calculate_network(*inputs, 1, -1, 0, 1, -1, 1)\n",
    "    \n",
    "mse = total_squared_errors/(len(input_values))\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_network(x1, x2, expected_output, w1, w2, w3, w4, w5, w6):\n",
    "    # ReLU activation function\n",
    "    def relu(x):\n",
    "        return max(0, x)\n",
    "    \n",
    "    H1 = relu(w1 * x1 + w3 * x2) # \n",
    "    H2 = relu(w4 * x1 + w2 * x2) # \n",
    "\n",
    "    O1 = w5 * H1 + w6 * H2\n",
    "    squared_error = (O1 - expected_output)**2\n",
    "    \n",
    "    return(squared_error)\n",
    "\n",
    "input_values = [\n",
    "    (0, 0, 0), #doesn't matter what the weights are\n",
    "    (0, 1, 1), \n",
    "    (1, 0, 1),\n",
    "    (1, 1, 0)\n",
    "]\n",
    "\n",
    "total_squared_errors = 0\n",
    "\n",
    "for inputs in input_values:\n",
    "    total_squared_errors += calculate_network(*inputs, -1, -1, 1, 1, 1, 1)\n",
    "    \n",
    "mse = total_squared_errors/(len(input_values))\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best weights: (-1, -1, 1, 1, 1, 1)\n",
      "Lowest MSE: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "def calculate_network(x1, x2, expected_output, w1, w2, w3, w4, w5, w6):\n",
    "    # ReLU activation function\n",
    "    def relu(x):\n",
    "        return max(0, x)\n",
    "    \n",
    "    # Hidden layer calculations\n",
    "    H1 = relu(w1 * x1 + w3 * x2)  # Neuron 1\n",
    "    H2 = relu(w4 * x1 + w2 * x2)  # Neuron 2\n",
    "\n",
    "    # Output layer calculation\n",
    "    O1 = w5 * H1 + w6 * H2\n",
    "    \n",
    "    # Compute squared error\n",
    "    squared_error = (O1 - expected_output) ** 2\n",
    "    \n",
    "    return squared_error\n",
    "\n",
    "# XOR input-output pairs\n",
    "input_values = [\n",
    "    (0, 0, 0),\n",
    "    (0, 1, 1),\n",
    "    (1, 0, 1),\n",
    "    (1, 1, 0)\n",
    "]\n",
    "\n",
    "# Define possible weight values (-1, 0, 1)\n",
    "possible_weights = [-1, 0, 1]\n",
    "\n",
    "# Initialize variables to store the best weights and the lowest MSE\n",
    "best_weights = None\n",
    "lowest_mse = float('inf')\n",
    "\n",
    "# Iterate over all possible combinations of weights\n",
    "for w1, w2, w3, w4, w5, w6 in product(possible_weights, repeat=6):\n",
    "    total_squared_errors = 0\n",
    "    \n",
    "    # Calculate the total squared error for this set of weights\n",
    "    for inputs in input_values:\n",
    "        total_squared_errors += calculate_network(*inputs, w1, w2, w3, w4, w5, w6)\n",
    "    \n",
    "    # Calculate the mean squared error (MSE)\n",
    "    mse = total_squared_errors / len(input_values)\n",
    "    \n",
    "    # Update the best weights if this combination has a lower MSE\n",
    "    if mse < lowest_mse:\n",
    "        lowest_mse = mse\n",
    "        best_weights = (w1, w2, w3, w4, w5, w6)\n",
    "\n",
    "# Output the best weights and the corresponding MSE\n",
    "print(f\"Best weights: {best_weights}\")\n",
    "print(f\"Lowest MSE: {lowest_mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A deep learning, neural network, based model would likely yield a better churn prediction model due to the abundance of data. In fact, this is a problem that I worked on during my time at [Webflow](webflow.com). Our customer volume was far less than the 100 million customers mentioned in this question. Due to the relatively \"low\" number of training data points I opted for a traditional Machine Learning model and spent about 3 months developing a feature engineering to create and test thousdands of features for training the model. Ultimately, the model proved to give us directional indicators for groups (aka clusters) of customers with a similar set of purchasing patterns and product usage, however, it wasn't as accurate as we would have liked. Additionally, due to the compute constraints we were unable to try a neural network approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions in the output layer would probably be a sigmoid function where we predict if a user churns or not (which would be similar to a standard output of a logistic regression model). The input layers would likely be.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this is a retail business, one way to reduce churn would be to offer discounts to the customers. However, this comes at the cost of impacting the financials and could decrease margins. Additionally, if \"word\" got out that these discounts were being offered to customers (via social media platforms) it would cause an uptick of existing customers headed towards the \"churn\" route in hopes of getting discounts. This is a traditional \"slippery slope\" problem and often has more risks than rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
